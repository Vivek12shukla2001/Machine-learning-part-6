{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFNbONdm1JCzyFpkNXi3hR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["theory\n","\n","1. Can we use Bagging for regression problems?\n","   Yes, Bagging can be used for regression problems. It is implemented using regressors like DecisionTreeRegressor in ensemble methods such as BaggingRegressor.\n","\n","2. What is the difference between multiple model training and single model training?\n","   Single model training involves training one model on the dataset, while multiple model training (ensemble) combines the predictions of several models to improve accuracy and reduce overfitting.\n","\n","3. Explain the concept of feature randomness in Random Forest.\n","   Random Forest introduces feature randomness by selecting a random subset of features at each split, ensuring diversity among trees and reducing correlation.\n","\n","4. What is OOB (Out-of-Bag) Score?\n","   OOB score is an internal validation method for Bagging and Random Forest. It evaluates model performance using samples not included in the bootstrap sample for training.\n","\n","5. How can you measure the importance of features in a Random Forest model?\n","   Feature importance can be measured by how much each feature decreases impurity across the forest or by permutation importance.\n","\n","6. Explain the working principle of a Bagging Classifier.\n","   A Bagging Classifier trains multiple base estimators (usually Decision Trees) on different bootstrap samples and aggregates their predictions via majority vote.\n","\n","7. How do you evaluate a Bagging Classifierâ€™s performance?\n","   You can evaluate it using metrics like accuracy, precision, recall, F1-score, or AUC on a test set.\n","\n","8. How does a Bagging Regressor work?\n","   It trains multiple regressors on different bootstrap samples and averages their predictions to get the final output.\n","\n","9. What is the main advantage of ensemble techniques?\n","   Ensemble techniques reduce variance and improve generalization by combining multiple models.\n","\n","10. What is the main challenge of ensemble methods?\n","    They can be computationally expensive and may lose interpretability.\n","\n","11. Explain the key idea behind ensemble techniques.\n","    The key idea is to combine multiple weak learners to form a strong learner with improved accuracy and robustness.\n","\n","12. What is a Random Forest Classifier?\n","    A Random Forest Classifier is an ensemble of Decision Trees using bagging and feature randomness to classify data.\n","\n","13. What are the main types of ensemble techniques?\n","    Bagging, Boosting, and Stacking are the main types of ensemble techniques.\n","\n","14. What is ensemble learning in machine learning?\n","    Ensemble learning combines predictions from multiple models to improve performance.\n","\n","15. When should we avoid using ensemble methods?\n","    Avoid them when interpretability is a key requirement or when computational resources are limited.\n","\n","16. How does Bagging help in reducing overfitting?\n","    Bagging reduces overfitting by averaging predictions from multiple models, thus smoothing out noise.\n","\n","17. Why is Random Forest better than a single Decision Tree?\n","    Random Forest reduces overfitting and increases accuracy by averaging the predictions of multiple trees.\n","\n","18. What is the role of bootstrap sampling in Bagging?\n","    Bootstrap sampling creates diverse training datasets by sampling with replacement, promoting model diversity.\n","\n","19. What are some real-world applications of ensemble techniques?\n","    Applications include fraud detection, recommendation systems, medical diagnosis, and spam filtering.\n","\n","20. What is the difference between Bagging and Boosting?\n","    Bagging builds models independently in parallel and reduces variance; Boosting builds models sequentially and reduces bias.\n"],"metadata":{"id":"KdN9Usf9iulw"}},{"cell_type":"code","source":["\n","\n","import numpy as np\n","from sklearn.datasets import load_breast_cancer, load_diabetes\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n","from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, mean_squared_error, roc_auc_score\n","\n","\n","#  Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n","def q1_bagging_classifier_decision_tree():\n","    X, y = load_breast_cancer(return_X_y=True)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","    bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n","    bag_clf.fit(X_train, y_train)\n","    y_pred = bag_clf.predict(X_test)\n","    print(\"Q1 - Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n","\n","\n","# ((2) Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n","def q2_bagging_regressor_decision_tree():\n","    X, y = load_diabetes(return_X_y=True)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","    bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n","    bag_reg.fit(X_train, y_train)\n","    y_pred = bag_reg.predict(X_test)\n","    mse = mean_squared_error(y_test, y_pred)\n","    print(\"Q2 - Bagging Regressor MSE:\", mse)\n","\n","\n","# (.2) Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n","def q3_random_forest_classifier_feature_importance():\n","    X, y = load_breast_cancer(return_X_y=True)\n","    rf_clf = RandomForestClassifier(random_state=42)\n","    rf_clf.fit(X, y)\n","    importances = rf_clf.feature_importances_\n","    print(\"Q3 - Random Forest Feature Importances:\")\n","    for i, imp in enumerate(importances):\n","        print(f\" Feature {i}: {imp:.4f}\")\n","\n","\n","# 2) Train a Random Forest Regressor and compare its performance with a single Decision Tree\n","def q4_random_forest_regressor_vs_decision_tree():\n","    X, y = load_diabetes(return_X_y=True)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","    dt_reg = DecisionTreeRegressor(random_state=42)\n","    dt_reg.fit(X_train, y_train)\n","    dt_pred = dt_reg.predict(X_test)\n","    dt_mse = mean_squared_error(y_test, dt_pred)\n","\n","    rf_reg = RandomForestRegressor(random_state=42)\n","    rf_reg.fit(X_train, y_train)\n","    rf_pred = rf_reg.predict(X_test)\n","    rf_mse = mean_squared_error(y_test, rf_pred)\n","\n","    print(f\"Q4 - Decision Tree Regressor MSE: {dt_mse:.4f}\")\n","    print(f\"Q4 - Random Forest Regressor MSE: {rf_mse:.4f}\")\n","\n","\n","# (\t2) Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n","def q5_random_forest_oob_score():\n","    X, y = load_breast_cancer(return_X_y=True)\n","    rf_clf = RandomForestClassifier(oob_score=True, random_state=42, n_estimators=100)\n","    rf_clf.fit(X, y)\n","    print(\"Q5 - Random Forest OOB Score:\", rf_clf.oob_score_)\n","\n","\n","# 2) Train a Bagging Classifier using SVM as a base estimator and print accuracy\n","def q6_bagging_classifier_svm():\n","    X, y = load_breast_cancer(return_X_y=True)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","    bag_svm = BaggingClassifier(estimator=SVC(probability=True), n_estimators=10, random_state=42)\n","    bag_svm.fit(X_train, y_train)\n","    y_pred = bag_svm.predict(X_test)\n","    print(\"Q6 - Bagging Classifier with SVM Accuracy:\", accuracy_score(y_test, y_pred))\n","\n","\n","# 2) Train a Random Forest Classifier with different numbers of trees and compare accuracy\n","def q7_random_forest_varying_trees():\n","    X, y = load_breast_cancer(return_X_y=True)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","    for n in [10, 50, 100, 200]:\n","        rf = RandomForestClassifier(n_estimators=n, random_state=42)\n","        rf.fit(X_train, y_train)\n","        acc = accuracy_score(y_test, rf.predict(X_test))\n","        print(f\"Q7 - Random Forest with {n} trees Accuracy: {acc:.4f}\")\n","\n","\n","# Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n","\n","def q8_bagging_classifier_logistic_regression_auc():\n","    X, y = load_breast_cancer(return_X_y=True)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","    bag_lr = BaggingClassifier(estimator=LogisticRegression(max_iter=1000), n_estimators=20, random_state=42)\n","    bag_lr.fit(X_train, y_train)\n","    y_prob = bag_lr.predict_proba(X_test)[:, 1]\n","    auc_score = roc_auc_score(y_test, y_prob)\n","    print(\"Q8 - Bagging Classifier with Logistic Regression AUC:\", auc_score)\n","\n","\n","# ('2) Train a Random Forest Regressor and analyze feature importance scores\n","def q9_random_forest_regressor_feature_importance():\n","    X, y = load_diabetes(return_X_y=True)\n","    rf_reg = RandomForestRegressor(random_state=42)\n","    rf_reg.fit(X, y)\n","    print(\"Q9 - Random Forest Regressor Feature Importances:\")\n","    for i, imp in enumerate(rf_reg.feature_importances_):\n","        print(f\" Feature {i}: {imp:.4f}\")\n","\n","\n","# .2 Train an ensemble model using both Bagging and Random Forest and compare accuracy\n","def q10_ensemble_bagging_random_forest_comparison():\n","    X, y = load_breast_cancer(return_X_y=True)\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","    bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n","    bag.fit(X_train, y_train)\n","    rf = RandomForestClassifier(n_estimators=50, random_state=42)\n","    rf.fit(X_train, y_train)\n","    bag_acc = accuracy_score(y_test, bag.predict(X_test))\n","    rf_acc = accuracy_score(y_test, rf.predict(X_test))\n","    print(f\"Q10 - Bagging Classifier Accuracy: {bag_acc:.4f}\")\n","    print(f\"Q10 - Random Forest Classifier Accuracy: {rf_acc:.4f}\")\n","\n","\n","if __name__ == \"__main__\":\n","    q1_bagging_classifier_decision_tree()\n","    print()\n","    q2_bagging_regressor_decision_tree()\n","    print()\n","    q3_random_forest_classifier_feature_importance()\n","    print()\n","    q4_random_forest_regressor_vs_decision_tree()\n","    print()\n","    q5_random_forest_oob_score()\n","    print()\n","    q6_bagging_classifier_svm()\n","    print()\n","    q7_random_forest_varying_trees()\n","    print()\n","    q8_bagging_classifier_logistic_regression_auc()\n","    print()\n","    q9_random_forest_regressor_feature_importance()\n","    print()\n","    q10_ensemble_bagging_random_forest_comparison()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zusvy3TGizi1","executionInfo":{"status":"ok","timestamp":1748832938298,"user_tz":-330,"elapsed":10785,"user":{"displayName":"prem sharma","userId":"08916668901930045136"}},"outputId":"49c0296f-be08-4072-b5d6-b47ed4f90ca2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Q1 - Bagging Classifier Accuracy: 0.958041958041958\n","\n","Q2 - Bagging Regressor MSE: 3096.3144324324335\n","\n","Q3 - Random Forest Feature Importances:\n"," Feature 0: 0.0348\n"," Feature 1: 0.0152\n"," Feature 2: 0.0680\n"," Feature 3: 0.0605\n"," Feature 4: 0.0080\n"," Feature 5: 0.0116\n"," Feature 6: 0.0669\n"," Feature 7: 0.1070\n"," Feature 8: 0.0034\n"," Feature 9: 0.0026\n"," Feature 10: 0.0143\n"," Feature 11: 0.0037\n"," Feature 12: 0.0101\n"," Feature 13: 0.0296\n"," Feature 14: 0.0047\n"," Feature 15: 0.0056\n"," Feature 16: 0.0058\n"," Feature 17: 0.0038\n"," Feature 18: 0.0035\n"," Feature 19: 0.0059\n"," Feature 20: 0.0828\n"," Feature 21: 0.0175\n"," Feature 22: 0.0808\n"," Feature 23: 0.1394\n"," Feature 24: 0.0122\n"," Feature 25: 0.0199\n"," Feature 26: 0.0373\n"," Feature 27: 0.1322\n"," Feature 28: 0.0082\n"," Feature 29: 0.0045\n","\n","Q4 - Decision Tree Regressor MSE: 5941.7027\n","Q4 - Random Forest Regressor MSE: 3010.1126\n","\n","Q5 - Random Forest OOB Score: 0.961335676625659\n","\n","Q6 - Bagging Classifier with SVM Accuracy: 0.951048951048951\n","\n","Q7 - Random Forest with 10 trees Accuracy: 0.9510\n","Q7 - Random Forest with 50 trees Accuracy: 0.9720\n","Q7 - Random Forest with 100 trees Accuracy: 0.9650\n","Q7 - Random Forest with 200 trees Accuracy: 0.9650\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["Q8 - Bagging Classifier with Logistic Regression AUC: 0.997919267582189\n","\n","Q9 - Random Forest Regressor Feature Importances:\n"," Feature 0: 0.0575\n"," Feature 1: 0.0119\n"," Feature 2: 0.2762\n"," Feature 3: 0.0871\n"," Feature 4: 0.0473\n"," Feature 5: 0.0554\n"," Feature 6: 0.0512\n"," Feature 7: 0.0271\n"," Feature 8: 0.3156\n"," Feature 9: 0.0708\n","\n","Q10 - Bagging Classifier Accuracy: 0.9580\n","Q10 - Random Forest Classifier Accuracy: 0.9720\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"ruwTOkmxj3ml"}}]}